"""
p_352_tts_dialog_parser.py - –ü–∞—Ä—Å–µ—Ä —Å—Ü–µ–Ω–∞—Ä—ñ—ó–≤ Multi Dialog –¥–ª—è TTS.
–†–æ–∑–±–∏–≤–∞—î —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞—Å—Ç–∏–Ω–∏ –∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º –æ–±–º–µ–∂–µ–Ω—å —Ç–æ–∫–µ–Ω—ñ–≤.
–ü—ñ–¥—Ç—Ä–∏–º—É—î —Ç–µ–≥–∏ #gN, —Å—É—Ñ—ñ–∫—Å–∏ —à–≤–∏–¥–∫–æ—Å—Ç—ñ —Ç–∞ SFX.
"""

import re
import unicodedata
import logging
from typing import Dict, Any, List, Tuple, Optional

import numpy as np

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç–∏ –¥–ª—è PL-BERT (–Ω–µ –ø–µ—Ä–µ–≤–∏—â—É–≤–∞—Ç–∏ 512 —Ç–æ–∫–µ–Ω—ñ–≤)
PLBERT_MAX = 512
PLBERT_SAFE = 480  # –ó–∞–ø–∞—Å –±–µ–∑–ø–µ–∫–∏ –ø–µ—Ä–µ–¥ –º–∞–∫—Å–∏–º—É–º–æ–º
HARD_MAX_TOKENS = 280  # –¶—ñ–ª—å–æ–≤–∏–π –±—é–¥–∂–µ—Ç —Ç–æ–∫–µ–Ω—ñ–≤ –Ω–∞ —à–º–∞—Ç–æ–∫
CHAR_CAP = 1200  # –ú–∞–∫—Å–∏–º—É–º —Å–∏–º–≤–æ–ª—ñ–≤ –Ω–∞ —à–º–∞—Ç–æ–∫
SPEAKER_MAX = 30


class DialogParser:
    """–ü–∞—Ä—Å–µ—Ä —Å—Ü–µ–Ω–∞—Ä—ñ—ó–≤ Multi Dialog –¥–ª—è TTS."""
    
    def __init__(self, app_context: Dict[str, Any]):
        self.app_context = app_context
        self.logger = logging.getLogger("DialogParser")
        self.tts_engine = app_context.get('tts_engine')
        self._tokenizer = self._init_tokenizer()
        self.logger.info("‚úÖ Dialog Parser —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–æ")
    
    def _init_tokenizer(self):
        """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è —Ç–æ–∫–µ–Ω—ñ–∑–∞—Ç–æ—Ä–∞."""
        try:
            from transformers import AutoTokenizer
            tokenizer = AutoTokenizer.from_pretrained("albert-base-v2")
            self.logger.debug("–¢–æ–∫–µ–Ω—ñ–∑–∞—Ç–æ—Ä Albert –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ")
            return tokenizer
        except Exception as e:
            self.logger.warning(f"–¢–æ–∫–µ–Ω—ñ–∑–∞—Ç–æ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π: {e}. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å—Å—è –Ω–∞–±–ª–∏–∂–µ–Ω—ñ —Ä–æ–∑—Ä–∞—Ö—É–Ω–∫–∏")
            return None
    
    def _token_length(self, text: str) -> int:
        """–û—Ü—ñ–Ω–∏—Ç–∏ –¥–æ–≤–∂–∏–Ω—É –≤ —Ç–æ–∫–µ–Ω–∞—Ö."""
        if self._tokenizer:
            try:
                return len(self._tokenizer.encode(text, add_special_tokens=True))
            except Exception:
                pass
        
        # –ö–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–∏–π fallback: 1 —Å–∏–º–≤–æ–ª ~ 1 —Ç–æ–∫–µ–Ω + 32 –¥–ª—è –∑–∞–ø–∞—Å—É
        return len(text) + 32
    
    def normalize_text(self, text: str) -> str:
        """
        –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è —Ç–µ–∫—Å—Ç—É –ë–ï–ó –∑–º—ñ–Ω —Å–∏–º–≤–æ–ª—É '+'.
        –°–∏–º–≤–æ–ª '+' –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –¥–ª—è –≤–∫–∞–∑–∞–Ω–Ω—è –Ω–∞–≥–æ–ª–æ—Å—É.
        """
        if not isinstance(text, str):
            return str(text) if text else ""
        
        # NFKC –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è
        text = unicodedata.normalize("NFKC", text).replace("\ufeff", "")
        
        # –£–Ω—ñ—Ñ—ñ–∫–∞—Ü—ñ—è –∞–ø–æ—Å—Ç—Ä–æ—Ñ—ñ–≤ —Ç–∞ —Ç–∏—Ä–µ
        text = (text.replace("'", "'")
                   .replace("'", "'")
                   .replace(" º", "'")
                   .replace(" ª", "'")
                   .replace(" π", "'")
                   .replace("‚Äî", "-")
                   .replace("‚Äì", "-")
                   .replace("‚àí", "-"))
        
        # –í–∏–¥–∞–ª–µ–Ω–Ω—è –Ω–µ–≤–∏–¥–∏–º–∏—Ö —Å–∏–º–≤–æ–ª—ñ–≤, –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è \n, \r, \t, '+'
        out = []
        for ch in text:
            if ch == '+':
                out.append(ch)
                continue
            
            cat = unicodedata.category(ch)
            if cat in ("Cf", "Cc") and ch not in ("\n", "\r", "\t"):
                continue
            out.append(ch)
        
        text = "".join(out)
        
        # NBSP ‚Üí –∑–≤–∏—á–∞–π–Ω–∏–π –ø—Ä–æ–±—ñ–ª
        text = text.replace("\u00A0", " ")
        
        # –û—á–∏—â–µ–Ω–Ω—è –ø—Ä–æ–±—ñ–ª—ñ–≤ –Ω–∞–≤–∫–æ–ª–æ –ø–µ—Ä–µ–Ω–æ—Å—ñ–≤
        text = re.sub(r"\s*\n\s*", "\n", text)
        
        return text.strip()
    
    def _split_sentence_safe(self, sent: str, max_tokens: int) -> List[str]:
        """–†–æ–∑–±–∏–≤–∞—î –Ω–∞–¥–¥–æ–≤–≥–µ —Ä–µ—á–µ–Ω–Ω—è –ø–æ —Å–ª–æ–≤–∞—Ö –±–µ–∑ –ø–æ—Ä—É—à–µ–Ω–Ω—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏."""
        parts, buf = [], []
        
        for tok in re.findall(r"\S+\s*|\s+", sent):
            buf.append(tok)
            if self._token_length("".join(buf)) > max_tokens:
                if len(buf) == 1:
                    # –ù–∞–≤—ñ—Ç—å –æ–¥–Ω–µ —Å–ª–æ–≤–æ –ø–µ—Ä–µ–≤–∏—â—É—î –ª—ñ–º—ñ—Ç - —Ä—ñ–∂–µ–º–æ –ø–æ —á–∞—Å—Ç–∏–Ω–∞—Ö
                    chunk = tok
                    while self._token_length(chunk) > max_tokens:
                        cut = max(64, int(len(chunk) * 0.7))
                        parts.append(chunk[:cut])
                        chunk = chunk[cut:]
                    buf = [chunk]
                else:
                    # –ü–æ–º—ñ—Ç–∏—Ç–∏ –æ—Å—Ç–∞–Ω–Ω—î —Å–ª–æ–≤–æ —Ç–∞ –Ω–∞–∫–æ–ø–∏—Ç–∏ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ
                    last = buf.pop()
                    parts.append("".join(buf).strip())
                    buf = [last]
        
        if buf:
            parts.append("".join(buf).strip())
        
        out = [p for p in parts if p]
        
        # –î–æ–¥–∞—Ç–∫–æ–≤–∞ —Å—Ç—Ä–∞—Ö–æ–≤–∫–∞: —Ä—ñ–∂–µ–º–æ –¥—É–∂–µ –¥–æ–≤–≥—ñ —à–º–∞—Ç–∫–∏
        safe = []
        for chunk in out:
            if len(chunk) <= CHAR_CAP and self._token_length(chunk) <= max_tokens:
                safe.append(chunk)
                continue
            
            frag = chunk
            while len(frag) > 0 and (self._token_length(frag) > max_tokens or len(frag) > CHAR_CAP):
                m = re.search(r'(.{200,}?[,;:])\s+', frag, flags=re.DOTALL)
                cut = m.end() if m else min(len(frag), max(300, len(frag)//2))
                safe.append(frag[:cut].strip())
                frag = frag[cut:].lstrip()
            
            if frag:
                safe.append(frag)
        
        return safe
    
    def split_to_parts(self, text: str, max_tokens: int = HARD_MAX_TOKENS) -> List[str]:
        """
        –†–æ–∑–±–∏–≤–∞—î —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞—Å—Ç–∏–Ω–∏ –∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º:
          - –õ—ñ–º—ñ—Ç—É —Ç–æ–∫–µ–Ω—ñ–≤ (max_tokens, –∑–∞–∑–≤–∏—á–∞–π 280)
          - –ú–∞–∫—Å–∏–º—É–º—É —Å–∏–º–≤–æ–ª—ñ–≤ (1200)
          - –ê–±–∑–∞—Ü—ñ–≤ —Ç–∞ —Ä–µ—á–µ–Ω—å
        
        Args:
            text: –í—Ö—ñ–¥–Ω–∏–π —Ç–µ–∫—Å—Ç
            max_tokens: –ú–∞–∫—Å–∏–º—É–º —Ç–æ–∫–µ–Ω—ñ–≤ –Ω–∞ —á–∞—Å—Ç–∏–Ω—É
        
        Returns:
            –°–ø–∏—Å–æ–∫ —á–∞—Å—Ç–∏–Ω —Ç–µ–∫—Å—Ç—É
        """
        text = self.normalize_text(text)
        chunks = []
        
        # –†–æ–∑–±–∏–≤–∞—î–º–æ –ø–æ –∞–±–∑–∞—Ü–∞—Ö
        for para in re.split(r"\n{2,}", text.strip()):
            para = para.strip()
            if not para:
                continue
            
            # –†–æ–∑–±–∏–≤–∞—î–º–æ –ø–æ —Ä–µ—á–µ–Ω–Ω—è—Ö (–ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ —Å–∏–º–≤–æ–ª—ñ–≤ –ø–µ—Ä–µ–¥ . ! ? ‚Ä¶)
            sents = re.split(r"(?<=[\.\!\?‚Ä¶])\s+", para)
            buf = []
            
            for s in sents:
                cand = (" ".join(buf + [s])).strip() if buf else s.strip()
                if not cand:
                    continue
                
                # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ –≤–º—ñ—â—É—î—Ç—å—Å—è –≤ –±—é–¥–∂–µ—Ç
                if self._token_length(cand) <= max_tokens and len(cand) <= CHAR_CAP:
                    buf.append(s)
                    continue
                
                # –Ø–∫—â–æ –Ω–∞–≤—ñ—Ç—å —Ä–µ—á–µ–Ω–Ω—è –¥–æ–≤—à–µ –±—é–¥–∂–µ—Ç—É - –¥—Ä–æ–±–∏–º–æ –π–æ–≥–æ
                if self._token_length(s) > max_tokens or len(s) > CHAR_CAP:
                    if buf:
                        chunks.append(" ".join(buf).strip())
                        buf = []
                    chunks.extend(self._split_sentence_safe(s, max_tokens))
                else:
                    if buf:
                        chunks.append(" ".join(buf).strip())
                    buf = [s]
            
            if buf:
                chunks.append(" ".join(buf).strip())
        
        # –§—ñ–Ω–∞–ª—å–Ω–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∫–æ–∂–Ω–æ–≥–æ —à–º–∞—Ç–∫–∞
        safe_final = []
        for c in chunks:
            if self._token_length(c) <= max_tokens and len(c) <= CHAR_CAP:
                safe_final.append(c)
            else:
                safe_final.extend(self._split_sentence_safe(c, max_tokens))
        
        return [c for c in safe_final if c]
    
    def parse_script_events(self, text: str, voices_flat: List[str]) -> List[dict]:
        """
        –ü–∞—Ä—Å–∏—Ç—å —Å—Ü–µ–Ω–∞—Ä—ñ–π —É —Å–ø–∏—Å–æ–∫ –ø–æ–¥—ñ–π.
        
        –§–æ—Ä–º–∞—Ç–∏ —Ç–µ–≥—ñ–≤:
          #gN[_slow|_fast|_slowNN|_fastNN]: —Ç–µ–∫—Å—Ç  ‚Üí voice –ø–æ–¥—ñ—è
          #<sfx_id>                                ‚Üí sfx –ø–æ–¥—ñ—è
        
        Args:
            text: –¢–µ–∫—Å—Ç —Å—Ü–µ–Ω–∞—Ä—ñ—é
            voices_flat: –°–ø–∏—Å–æ–∫ –≥–æ–ª–æ—Å—ñ–≤ (–¥–ª—è –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó)
        
        Returns:
            –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–Ω–∏–∫—ñ–≤ –ø–æ–¥—ñ–π
        """
        events: List[dict] = []
        if not isinstance(text, str):
            return events
        
        lines = self.normalize_text(text).splitlines()
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è voice –ø–æ–¥—ñ–π: #g1_fast: —Ç–µ–∫—Å—Ç
        voice_pat = re.compile(
            r"^#g\s*([1-9]|[12][0-9]|30)(?:_((?:slow|fast)(?:\d{1,3})?))?\s*:??\s+(.*)$",
            re.IGNORECASE
        )
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è SFX –ø–æ–¥—ñ–π: #bell_sound
        sfx_pat = re.compile(r'^#([A-Za-z0-9_]+)\s*$', re.IGNORECASE)
        
        for line_no, raw_ln in enumerate(lines, start=1):
            ln = raw_ln.strip()
            if not ln:
                continue
            
            # –ü–µ—Ä–µ–≤—ñ—Ä–∞ voice —Ç–µ–≥—É
            m_voice = voice_pat.match(ln)
            if m_voice:
                g_str, suffix, text_body = m_voice.groups()
                g_num = int(g_str)
                suffix = suffix.lower() if suffix else ""
                
                if not text_body.strip():
                    raise RuntimeError(f"–ü–æ—Ä–æ–∂–Ω—ñ–π —Ç–µ–∫—Å—Ç –ø—ñ—Å–ª—è —Ç–µ–≥–∞ #g{g_num} –Ω–∞ —Ä—è–¥–∫—É {line_no}")
                
                if g_num < 1 or g_num > SPEAKER_MAX:
                    raise RuntimeError(f"–ù–µ–ø—Ä–∏–ø—É—Å—Ç–∏–º–∏–π –Ω–æ–º–µ—Ä —Å–ø—ñ–∫–µ—Ä–∞: {g_num} –Ω–∞ —Ä—è–¥–∫—É {line_no}")
                
                events.append({
                    "type": "voice",
                    "g": g_num,
                    "suffix": suffix,
                    "text": text_body
                })
                continue
            
            # –ü–µ—Ä–µ–≤—ñ—Ä–∞ SFX —Ç–µ–≥—É
            m_sfx = sfx_pat.match(ln)
            if m_sfx:
                sfx_id = m_sfx.group(1)
                
                # –í–∞–ª—ñ–¥–∞—Ü—ñ—è SFX —É –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó
                sfx_handler = self.app_context.get('sfx_handler')
                if sfx_handler and not sfx_handler.validate_sfx_id(sfx_id):
                    raise RuntimeError(f"SFX '{sfx_id}' –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ —É –∫–æ–Ω—Ñ—ñ–≥—É sfx.yaml (—Ä—è–¥–æ–∫ {line_no})")
                
                events.append({
                    "type": "sfx",
                    "id": sfx_id,
                    "params": {}
                })
                continue
            
            # –ö–æ–º–µ–Ω—Ç–∞—Ä—ñ (—Ä—è–¥–∫–∏, —â–æ –ø–æ—á–∏–Ω–∞—é—Ç—å—Å—è –∑ #)
            if ln.startswith('#'):
                continue
            
            # –ó–≤–∏—á–∞–π–Ω–∏–π —Ç–µ–∫—Å—Ç –±–µ–∑ —Ç–µ–≥—É ‚Üí —Å–ø—ñ–∫–µ—Ä #g1
            events.append({
                "type": "voice",
                "g": 1,
                "suffix": "",
                "text": ln
            })
        
        return events
    
    def compute_speed_effective(self, g_num: int, suffix: str, 
                               speeds_flat: List[float], ignore_speed: bool = False) -> float:
        """
        –û–±—á–∏—Å–ª—é—î –µ—Ñ–µ–∫—Ç–∏–≤–Ω—É —à–≤–∏–¥–∫—ñ—Å—Ç—å –¥–ª—è voice-–ø–æ–¥—ñ—ó.
        
        –ü—Ä—ñ–æ—Ä–∏—Ç–µ—Ç:
          1) –Ø–∫—â–æ ignore_speed=True ‚Üí DEFAULT_SPEED
          2) –Ø–∫—â–æ suffix='slow' –∞–±–æ 'fast' ‚Üí 0.80 –∞–±–æ 1.20
          3) –Ø–∫—â–æ suffix='slow95' ‚Üí 0.95
          4) –Ü–Ω—à–æ–º–æ–º—ñ—Ä–Ω–æ ‚Üí speeds_flat[g_num]
        """
        if ignore_speed:
            return 0.88  # –î–µ—Ñ–æ–ª—Ç, –º–æ–∂–Ω–∞ –æ—Ç—Ä–∏–º–∞—Ç–∏ –∑ –∫–æ–Ω—Ñ—ñ–≥—É
        
        suf = suffix.lower() if suffix else ""
        
        if suf == 'slow':
            return 0.80
        if suf == 'fast':
            return 1.20
        
        # –ü–∞—Ä—Å–∏–Ω–≥ slow95, fast110
        if suf.startswith('slow') and len(suf) > 4:
            try:
                val = float(suf[4:]) / 100.0
                return val
            except Exception:
                pass
        
        if suf.startswith('fast') and len(suf) > 4:
            try:
                val = float(suf[4:]) / 100.0
                return val
            except Exception:
                pass
        
        # –í–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ –∑–Ω–∞—á–µ–Ω–Ω—è —Å–ª–∞–π–¥–µ—Ä–∞
        if 1 <= g_num <= len(speeds_flat):
            try:
                return float(speeds_flat[g_num - 1])
            except Exception:
                pass
        
        return 0.88


def prepare_config_models():
    """–ü–æ–≤–µ—Ä—Ç–∞—î –º–æ–¥–µ–ª—ñ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó."""
    return {}


def initialize(app_context: Dict[str, Any]) -> DialogParser:
    """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è Dialog Parser."""
    logger = app_context.get('logger', logging.getLogger("DialogParser"))
    logger.info("üìù –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –ø–∞—Ä—Å–µ—Ä–∞ –¥—ñ–∞–ª–æ–≥—ñ–≤...")
    
    parser = DialogParser(app_context)
    app_context['dialog_parser'] = parser
    
    logger.info("‚úÖ Dialog Parser –≥–æ—Ç–æ–≤–∏–π")
    return parser


def stop(app_context: Dict[str, Any]) -> None:
    """–ó—É–ø–∏–Ω–∫–∞ Dialog Parser."""
    if 'dialog_parser' in app_context:
        del app_context['dialog_parser']
    
    logger = app_context.get('logger')
    if logger:
        logger.info("Dialog Parser –∑—É–ø–∏–Ω–µ–Ω–æ")
